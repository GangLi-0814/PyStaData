## 5. 多元线性回归

### 5.1 二元线性回归

使用一元线性回归会存在遗漏变量的问题，所以需要纳入更多的解释变量。

假设二元线性回归模型：
$$
y_i = \alpha + \beta x_{i1} + \gamma x_{i2} + \epsilon_{i} \quad (i = 1,...,n)
$$
$x_{i1}$ 与 $x_{i2}$ 为解释变量，$\alpha$ 为截距项；

$\beta$ 为在给定 $x_2$ 条件下，$x_1$ 对 $y$ 的边际效应（忽略扰动项 $\epsilon_i$）。

$\gamma$ 为在给定 $x_1$ 条件下，$x_2$ 对 $y$ 的边际效应。

**最优化问题：残差平方和最小**
$$
\min_{\hat \alpha, \hat \beta, \hat \gamma} \sum_{i=1}^n e_{i}^2 = \sum_{i=1}^n (y_i -\hat \alpha-\hat \beta x_{i1} -\hat \gamma x_{i2})^2
$$
寻找一个回归平面 $\hat y_{i} = \hat \alpha + \hat \beta x_{1i} + \hat \gamma x_{2i}$ ，即估计参数 $\hat \alpha, \hat \beta, \hat \gamma$ ，使得所有样本点 $\{(x_{i1},x_{2i},y_i)\}_{i=1}^n$ 离此回归平面最近。

求解：分别对 $\hat \alpha, \hat \beta, \hat \gamma$ 求偏导数，可得最小化的一阶条件，求解可得 $\hat \alpha, \hat \beta, \hat \gamma$ 的 OLS 估计量。

### 5.2 多元线性回归

基本形式：
$$
y_i = \beta_1 + \beta_2 x_{i2} + ... +\beta_Kx_{iK} + \epsilon_{i} \quad (i=1,...,n)
$$

$x_{i1}$ 为个体 $i$ 的第 1 个解释变量，$x_{i2}$ 为个体 $i$ 的第 2 个解释变量，以此类推。上式中，令 $x_{i1} \equiv 1$ （恒等于 1 ），代表常数项。 

**采用矩阵形式**，可将原模型写成：
$$
y_i = (\begin{matrix} 1 & x_{i2} & ... & x_{ik} \end{matrix}) \begin{Bmatrix} \beta_1 \\ \beta_2 \\ \vdots \\ \beta_K \end{Bmatrix} + \epsilon_i = \mathbf{x_{i}^\mathrm{'}} \mathbf{\beta} + \epsilon_i
$$

将所有这 $n$ 个方程叠放：
$$
\begin{Bmatrix}
y_1 = x_1'\beta+\epsilon_1 \\
y_2 = x_2'\beta+\epsilon_2 \\
\vdots \\
y_n = x_n'\beta+\epsilon_n
\end{Bmatrix}
$$
将共同的参数向量 $\mathbf{\beta}$ 向右边提出：
$$
\mathbf{y} = 
\begin{Bmatrix}
y_1 \\ y_2 \\ \vdots \\y_n
\end{Bmatrix}=
\underbrace{
\begin{Bmatrix}
x_1' \\ x_2' \\ \vdots \\x_n'
\end{Bmatrix}
}_{X}\beta+
\underbrace{
\begin{Bmatrix}
\epsilon_1 \\ \epsilon_2 \\ \vdots \\\epsilon_n
\end{Bmatrix}
}_{\epsilon}
=\mathbf{X \beta + \epsilon}
$$
其中，$y=(\begin{matrix} y_1 & y_2 & \cdots & y_n \end{matrix})'$ 为**被解释变量**构成的列向量；

$\epsilon=(\begin{matrix} \epsilon_1 & \epsilon_2 & \cdots & \epsilon_n \end{matrix})'$ 为**所有扰动项**构成的列向量；

$\mathbf{X}$ 为 $n \times K$ 数据矩阵。其第 $i$ 行包含全体 $i$ 的全部解释变量，而第 $k$ 列包含第 $k$ 个解释变量的全部观测值，即
$$
X = 
\begin{Bmatrix} 
1 & x_{12} & \cdots & x_{1K} \\
1 & x_{22} & \cdots & x_{2K} \\
\cdots & \cdots & \cdots & \cdots \\
1 & x_{n2} & \cdots & x_{nK}
\end{Bmatrix}_{n \times K}
$$

### 5.3 OLS 估计量的推导

对于多元回归模型，OLS 估计量的最小化问题为：
$$
\min_{\hat \beta_1, \cdots,\hat \beta_K} \quad \sum_{i=1}^n e_i^2 = \sum_{i=1}^n (y_i - \hat \beta_1 - \hat \beta_2 x_{i2} - \hat \beta_3 x_{i3} - \cdots - \hat \beta_K x_{iK})^2
$$
一阶条件为：
$$
\begin{cases}
\frac{\delta}{\delta \hat \beta_1} \sum_{i=1}^n e_i^2 = -2 \sum_{i=1}^n (y_i - \hat \beta_1 - \hat \beta_2 x_{i2} - \cdots - \hat \beta_K x_{iK}) = 0 \\
\frac{\delta}{\delta \hat \beta_2} \sum_{i=1}^n e_i^2 = -2 \sum_{i=1}^n (y_i - \hat \beta_1 - \hat \beta_2 x_{i2} - \cdots - \hat \beta_K x_{iK})x_{i2} = 0  \\
\qquad \qquad \qquad \qquad \qquad \qquad\qquad\vdots \\
\frac{\delta}{\delta \hat \beta_K} \sum_{i=1}^n e_i^2 = -2 \sum_{i=1}^n (y_i - \hat \beta_1 - \hat \beta_2 x_{i2} - \cdots - \hat \beta_K x_{iK})x_{iK} = 0  
\end{cases}
$$
消去方程左边的 “-2” 可得：
$$
\begin{cases}
\sum_{i=1}^n (y_i - \hat \beta_1 - \hat \beta_2 x_{i2} - \cdots - \hat \beta_K x_{iK}) = 0 \\
\sum_{i=1}^n x_{i2} (y_i - \hat \beta_1 - \hat \beta_2 x_{i2} - \cdots - \hat \beta_K x_{iK}) = 0  \\
\qquad \qquad \qquad \qquad\qquad\vdots \\
\sum_{i=1}^n x_{iK}(y_i - \hat \beta_1 - \hat \beta_2 x_{i2} - \cdots - \hat \beta_K x_{iK}) = 0  
\end{cases}
$$
这是包含 $K$ 个未知数 $(\hat \beta_1, \hat \beta_2, \cdots, \hat \beta_K)$ 与 $K$ 个方程的联立方程组，称为 “正规方程组” （normal equations）。

满足此正规方程组的 $\hat \beta \equiv \begin{matrix}(\hat \beta_1 & \hat \beta_2 & \cdots & \hat \beta_K) \end{matrix}$ 称为 OLS 估计量。

由于残差 $e_i \equiv y_i - \hat \beta_1 - \hat \beta_2 x_{i2} - \cdots - \hat \beta_Kx_{iK}$ ，故正规方程组可写为：
$$
\begin{cases}
\sum_{i=1}^n e_i = 0 \\
\sum_{i=1}^n x_{i2}e_i = 0 \\
\qquad\quad  \vdots\\
\sum_{i=1}^n x_{iK}e_i = 0 
\end{cases}
$$
上式每一方程都是乘积求和的形式，可用向量内积表示。

第 1 个方程可写为：
$$
\sum_{i=1}^n e_i = \begin{matrix}(1&1&\cdots&1)\end{matrix}\begin{Bmatrix}e_1 \\e_2\\\vdots\\e_n\end{Bmatrix} = 0
$$
第 2 个方程可写为：
$$
\sum_{i=1}^n x_{i2}e_i = \begin{matrix}(x_{12}&x_{22}&\cdots&x_{n2})\end{matrix}\begin{Bmatrix}e_1 \\e_2\\\vdots\\e_n\end{Bmatrix} = 0
$$
依此类推，第 K 个方程可写为：
$$
\sum_{i=1}^n x_{iK}e_i = \begin{matrix}(x_{1K}&x_{2K}&\cdots&x_{nK})\end{matrix}\begin{Bmatrix}e_1 \\e_2\\\vdots\\e_n\end{Bmatrix} = 0
$$
残差向量 $e \equiv \begin{matrix}(e_1&e_2&\cdots&e_n)\end{matrix}$ 与每个解释变量都正交，这是 OLS 估计量的一大特征。将以上内积以矩阵形式表示：
$$
\underbrace{
\begin{Bmatrix}
1 & 1 & \cdots & 1 \\
x_{12}&x_{22}&\cdots&x_{n2} \\
\cdots &\cdots&\cdots&\cdots \\
x_{1K}&x_{2K}&\cdots&x_{nK}
\end{Bmatrix}
}_{X'}
\underbrace{
\begin{Bmatrix}
e_1 \\
e_2 \\
\vdots \\
e_{n}
\end{Bmatrix}
}_{e}
=
\underbrace{
\begin{Bmatrix}
0 \\
0 \\
\vdots \\
0
\end{Bmatrix}
}_{0}
$$
$X'$ 为数据矩阵 $X$ 的转置。正规方程组可简洁的写为：
$$
X'e = 0
$$
从 $e_i = y_i - (\hat \beta_1 + \hat \beta_2 x_{i2} + \hat \beta_3 x_{i3} + \cdots + \hat \beta_K x_{iK})$ 出发，可将残差向量写为：
$$
e = y - X \hat \beta
$$
代入正规方程组可得：
$$
X'(y-X\hat \beta) = 0
$$
乘开并移项可知，最小二乘估计量 $\hat \beta$ 满足：
$$
(X'X)_{K \times K} \hat \beta_{K \times 1} = X'_{K\times n} y_{n \times 1}
$$
假设 $(X'X)^{-1}$ 存在，求解 OLS 估计量：
$$
\mathbf{\hat \beta = (X^\mathrm{'}X)^{-1}X^\mathrm{'}y}
$$

这就是多元回归 OLS 的估计量。

### 5.4 OLS 的几何解释

<img src="/Users/gangli/PyStaData/PhD/Econometrics/陈强-计量经济学及Stata应用/images/5-1.png" style="zoom:50%;" />

拟合值 $\mathbf{\hat y}$ 可视为被解释变量 $\mathbf{y}$ 向解释变量超平面 $\mathbf{X}$ 的投影 (projection)。**由于拟合值为解释变量的线性组合，即 $\mathbf{\hat y  = X \hat β}$ ，故拟合值向量  $\mathbf{\hat y}$ 正好在超平面 $\mathbf{X}$  上**。根据 OLS 的正交性，残差向量 $\mathbf{e}$ 与 $\mathbf{\hat y}$ 正交 。

### 5.5 拟合优度

#### 5.5.1 拟合优度

由于OLS的正交性，平方和分解公式依然成立：
$$
\underbrace{
\sum_{i=1}^n (y_i-\bar y)^2
}_{TSS}
= 
\underbrace{
\sum_{i=1}^n(\hat y_i-\bar y)^2
}_{ESS}
+ 
\underbrace{
\sum_{i=1}^n e_{i}^2
}_{RSS}
$$
**拟合优度的缺点是：如果增加解释变量的数目，则 $R^2$ 只增不减，因为至少可让新增解释变量的系数为 0 而保持 $R^2$ 不变**。

另一方面，通过最优地选择新增解释变量的系数（以及已有解释变量的系数），通常可以提高 $R^2$ 。

#### 5.5.2 调整拟合优度

引入调整 $R^2$ 对解释变量过多（模型不够简洁）进行惩罚。

定义调整 $\bar R^2$ 为：
$$
\bar R^2 = 1 - \frac{\sum_{i=1}^n e_{i}^2/(n-K)}{\sum_{i=1}^n(y_i-\bar y)^2/(n-1)}
$$

$\sum_{i=1}^n e_{i}^2$ 的自由度（degree of freedom）为$(n-K)$；$\sum_{i=1}^n(y_i-\bar y)^2$ 的自由度为 $(n-1)$ 。

---

【问题】为何 $\sum_{i=1}^n e_{i}^2$ 的自由度（degree of freedom）为$(n-K)$ ?

虽然  $\sum_{i=1}^n e_{i}^2$ 由 $n$ 个随机变量 $\{e_1,...,e_n\}$ 所构成，但 $\{e_1,...,e_n\}$ 受由 $K$ 个方程组成的正规方程组的约束，故只有其中 $(n-K)$ 个残差是（自由）独立的。

【问题】为何 $\sum_{i=1}^n(y_i-\bar y)^2$ 的自由度为 $(n-1)$  ?

虽然 $\sum_{i=1}^n(y_i-\bar y)^2$ 由 $n$ 个离差 $\{(y_1 - \bar y), ..., (y_n - \bar y)\}$ 所构成，但这些离差和必然为 0 ，即 $\sum_{i=1}^n (y_i - \bar y) =0$，故只有其中 $(n-1)$ 个离差是（自由）独立的。 

---

如果让 $K$ 增多，有两个相反方向的变动。

**$\bar R^2$ 的缺点是可能为负。**

**$R^2$ 和 $\bar R^2$ 只能反映拟合优度的好坏，评估回归方程是否显著应该使用 F 检验。**

如果回归模型无常数项，则仍需使用“非中心$R^2$（uncentered $R^2$）：
$$
R_{uc}^2 \equiv \frac{\sum_{i=1}^n \hat y_{i}^2}{\sum_{i=1}^n y_{i}^2}
$$

### 5.6 古典线性回归模型的假定

**SLR.1 线性（linearity）假定**

每个解释变量对 $y_i$ 的**边际效应**为常数。如 $\frac{\delta y_i}{\delta x_{i2}}=\beta_2$ （忽略扰动项 $\epsilon_i$）。

在现实中，如果边际效应可变，可以加入平方项（如 $x_{i2}^2$）或交叉项（$x_{i2}x_{i3}$）。

（平方项）考虑回归方程：
$$
\ln{w_i} = \beta_1 + \beta_2s_i+\beta_3s_{i}^2+\epsilon_i
$$

$\ln{w_i}$ 为工资对数，$s_i$ 为教育年限。教育年限对工资对数的边际效应为：
$$
\frac{\delta \ln{w_i}}{\delta s_i} = \beta_2 + 2 \beta_3s_i
$$
如果 $\beta_3 < 0 $，则存在教育投资回报率递减；反之，则存在教育投资回报率递增。

（交叉项）考虑生产函数方程：

$$
y_i = \beta_1 + \beta_2k_i + \beta_3 l_i + \beta_4 k_i \times l_i + \epsilon_i
$$

$y$ 为产出，$k$ 为资本，$l$为劳动力， $k \times l$ 为资本与劳动力的互动项。劳动的边际产出为$\frac{\delta y_i}{\delta l_i} = \beta_3 + \beta_4k_i$ （忽略扰动项）。
如果 $\beta_4 > 0$ 则说明资本与劳动力是互补的，即随着资本上升，劳动力的边际产出也增加。

**线性假定的本质要求是，回归函数是参数（$\beta_1, ...,\beta_k$）的线性函数。**

**SLR.2 严格外生性（strict exogeneity）要求**
$$
E(\epsilon_i | X) = E(\epsilon_i | x_1, \cdots, x_n) = 0 \quad (i = 1, \cdots, n)
$$
严格外生性意味着在给定数据矩阵 $\mathbf{X}$ 的情况下，扰动项 $\epsilon_{i}$ 的条件期望为 0 。

---

【定义】如果随机变量 $x, y$ 满足 $E(xy) = 0$，则称 $x,y$ 正交（orthogonal）。

根据此定义，可证明解释变量与扰动项正交。因为：
$$
0 = Cov(x_{jk}, \epsilon_i) = E(x_{jk}\epsilon_i) - E(x_{jk})\underbrace{E(\epsilon_i)}_{=0} = E(x_{jk}\epsilon_i)
$$

---

**SLR.3 不存在严格多重共线性（strict multicollinearity）**

即数据矩阵 $X$ 满列秩（full column rank）。数据矩阵的各列向量为线性无关，即不存在某个解释变量为另一解释变量的倍数，或可由解释变量线性表现的情形。

### 5.7 OLS 的小样本性质

在满足 SLR.1~SLR.3 的条件下， OLS 估计量具有以下良好性质：

- 线性性：OLS 估计量 $\mathbf{\hat \beta}$为线性估计量；
- 无偏性：$E(\hat \beta | X) = \beta$，即 $\hat \beta$ 不会系统地高估或低估 $\beta$；
- 估计量 $\hat \beta$ 的协方差矩阵

---

【证明】无偏性

抽样误差为：

$\hat \beta - \beta = (X'X)^{-1}X'y - \beta = (X'X)^{-1}X'(X\beta+\epsilon)-\beta=(X'X)^{-1}X'\epsilon \equiv A \epsilon$

记 $A \equiv (X'X)^{-1}X'$ 。给定解释变量 $X$ ，对上式两边求条件期望，根据严格外生性可得：
$$
E(\hat \beta -\beta | X) = E(A\epsilon|X) = A \underbrace{E(\epsilon|X)}_{=0} = 0
$$
移项可得：$E(\hat \beta |X) = \beta$ 。

在此证明中，严格外生性不可或缺。

使用迭代期望定律，可进一步证明，无条件期望 $E(\hat \beta) = \beta$ ，因为：
$$
E(\hat \beta) = E_{X}E(\hat \beta|X) = E_{X}(\beta) = \beta
$$

---

**SLR.4 球型扰动项（spherical disturbance）假定**

球型扰动项（spherical disturbance）满足同方差、无自相关性质，扰动项 $\epsilon$ 的协方差矩阵可写为：
$$
Var(\epsilon|X) = \sigma^2I_{n} = 
\begin{Bmatrix} 
\sigma^2 & 0 & \cdots & 0 \\
0 & \sigma^2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma^2
\end{Bmatrix}
$$
协方差矩阵 $Var(\epsilon |X)$ 的主对角线元素都等于 $\sigma^2$ ，即满足条件同方差（同方差），若不满足则存在异方差。

协方差矩阵 $Var(\epsilon |X)$ 的非主对角线元素都等于 0 ，故不同个体的扰动项之间无自相关，反之则存在自相关。

球型扰动项假定是证明协方差表达式 $\sigma^2(X'X)^{-1}$ 的关键（无偏性不依赖于球型扰动项）。

**高斯-马尔克夫定理：在 SLR.1~SLR.4 之下，最小二乘法是最佳无偏估计量（Best Linear Unbiased Estimator, 简记BLUE），即在所有线性的无偏估计中，最小二乘法的方差最小。**

### 5.8 对单个系数 t 检验

**SLR.5 在给定 $X$ 的情况下，$\epsilon|X$ 的条件分布为正态，即 $\epsilon|X \sim N(0, \sigma^2I_n)$ 。**

**假设检验是一种概率意义上的反证法。**首先假设原假设成立，然后看在原假设成立的前提下，是否导致不太可能发生“小概率事件”在一次抽样的样本中出现。

---

【定理】t 统计量的分布

在 SLR.1~SLR.5 均满足，且原假设 $H_0:\beta_k =c$ 也成立的情况下，t 统计量服从自由度为 $(n-K)$ 的 t 分布：
$$
t_k \equiv \frac{\hat \beta_k-c}{SE(\hat \beta_k)} \sim t(n-K)
$$
$SE(\hat \beta_k) \equiv \sqrt{s^2(X'X)_{kk}^{-1}}$ 为 $\hat \beta_k$ 的标准误。

更一般地，t 统计量的通用公式为：
$$
t = \frac{估计量-假想值}{估计量的标准误}
$$

---

#### 5.8.1 t 检验的步骤

**第一步：**计算 t 统计量 $t_k = \frac{\hat \beta_k-c}{SE(\hat \beta_k)} \sim t(n-K)$，记其具体取值为 $t_k$ 。
**第二步：**计算显著性水平为 $\alpha$ 的临界值 $t_{\alpha/2}(n-K)$ ，其中 $t_{\alpha/2}(n-K)$ 的定义为：
$$
P\{T > t_{\alpha/2}(n-K) \} = P\{T < t_{\alpha/2}(n-K)\} = \alpha/2
$$

其中，随机变量 $T \sim t(n-K)$ 。

随机变量 $T$ 大于 $t_{\alpha/2}(n-K)$ ，或小于 $-t_{\alpha/2}(n-K)$ 的概率都是 $\alpha/2$，如下图：

<img src="/Users/gangli/PyStaData/PhD/Econometrics/陈强-计量经济学及Stata应用/images/5-6.png" style="zoom:50%"/>

通常取 $\alpha=5\%$，则 $\alpha/2 = 2.5\%$，有时也使用 $\alpha = 1\%$ 或  $\alpha = 10\%$ 。

**第三步：**如果 $|t_k| \geq t_{\alpha/2}(n-K)$ ，则 $t_k$ 落入“拒绝域”，故拒绝 $H_0$ 。反之，如果 $|t_k| < t_{\alpha/2}(n-K)$，则 $t_k$ 落入接受域，故接受 $H_0$ 。 

#### 5.8.2 计算 p 值

【定义】**称原假设可被拒绝的最小显著性水平为此假设检验问题的 p 值。**p 值越小，则越倾向于拒绝原假设。如选定显著性水平为 5% ，只要 p 值比 0.05 小，即可拒绝原假设。

在双边 $t$ 检验中，给定 $t$ 统计量的样本观测值 $t_k$ ，此假设检验问题的 p 值为：
$$
p值 \equiv P(|T|>|t_k|)
$$
其中，随机变量 $T \sim t(n-K)$ 。

给定 t 统计量 $t_k$ ，则 p 值衡量比 $|t_k|$ 更大的 t 分布两端的尾部概率，如下图：

<img src="/Users/gangli/PyStaData/PhD/Econometrics/陈强-计量经济学及Stata应用/images/5-7.png" style="zoom:70%"/>

如果 p 值为 0.05 ，则正好可以在 5% 的显著性水平上拒绝原假设，但无法在 4.9% 的显著性水平上拒绝原假设。

#### 5.8.3 计算置信区间

置信区间：参数最可能的取值范围。

假设置信区间为 $(1-\alpha)$ （比如 $\alpha = 5\%$，则 $1-\alpha=95\%$），即要找到置信区间，使得该区间覆盖真实参数 $\beta_k$ 的概率为 $(1-\alpha)$ 。

由于 $t_k = \frac{\hat \beta_k - \beta_k}{SE(\hat \beta_k)} \sim t(n-K)$ ，故 t 统计量落入接受域的概率为 $(1-\alpha)$ ：
$$
P \{-t_{\alpha/2} < \frac{\hat \beta_k - \beta_k}{SE(\hat \beta_k)} < t_{\alpha/2}\} = 1 - \alpha
$$
其中，$t_{\alpha/2}$ 为显著性水平为 $\alpha$ 的临界值。将不等式变形可得：
$$
[\hat \beta_k - t_{\alpha/2}SE(\hat \beta_k),\hat \beta_k + t_{\alpha/2}SE(\hat \beta_k) ]
$$
此置信区间以点估计 $\hat \beta_k$ 为中心，区间半径为 $t_{\alpha/2}SE(\hat \beta_k)$ 。

<img src="/Users/gangli/PyStaData/PhD/Econometrics/陈强-计量经济学及Stata应用/images/5-8.png" style="zoom:70%"/>

标准误 $SE(\hat \beta_k)$ 越大，对 $\beta_k$ 的估计越不准确，置信区间也越宽。

置信区间是随机区间，随着样本不用而不同。**如果置信区间为 95% ，抽样 100 次，得到 100 个置信区间，大约 95 个置信区间能覆盖到真实参数 $\beta_k$ 。**

#### 5.8.4 单边检验

#### 5.8.5 第 I 类错误与第 II 类错误

第 I 类错误：虽然原假设为真，但却根据观测数据做出了拒绝原假设的错误判断，即为“弃真”。

第 II 类错误：虽然原假设为假，但却根据观测数据做出了接受原假设的错误判断，即为“存伪”。

### 5.9 对线性假设的 F 检验

### 5.10  F 统计量的似然比原理表达式

### 5.11 预测

### 5.12 用 Stata 进行多元回归的实例

#### 5.12.1 用Stata进行二元回归

```Stata
* 柯布道格拉斯生产函数
use cobb_douglas.dta, clear
list

regress lny lnk lnl
predict lny1
predict e, res

list lny lny1 e
line lny lny1 year, lp(solid dash)
```

#### 5.12.2 多元回归的 Stata 实例

```Stata
* 多元回归
use ${d}/grilic.dta, clear
reg lnw s expr tenure smsa rns

vce // 回归系数的协方差矩阵
reg lnw s expr tenure smsa rns, noc //无常数项回归
reg lnw s expr tenure smsa if rns // only rns == 1
reg lnw s expr tenure smsa if ～rns // only rns == 0
reg lnw s expr tenure smsa rns if s>=12 // only 中学以上
reg lnw s expr tenure smsa if s>=12 & rns // 中学以上且住在南方

quietly reg lnw s expr tenure smsa rns
predict lnw1 // 拟合值
predict e, residual //残差
```

#### 5.12.3 假设检验

```Stata
* t 检验
test s = 0.1 // H_0: \beta_2 = 0.1
/*
 ( 1)  s = .1
       F(  1,   752) =    0.20
            Prob > F =    0.6515
解读：由于 t 分布的平方为 F 分布，故 Stata 统一汇报 F 统计量及其 p 值。 
上表显示，p 值 = 0.6515 ，故无法拒绝原假设。
*/

*手工计算 t 统计量
/*
t = \frac{估计值 - 假想值}{估计值的标准误} = \frac{0.102643 - 0.1}{0.0058488} = 0.45188757 \sim t(n-K) = t
*/

dis (0.102643 - 0.1)/0.0058488 // 0.45188757
ttable 752 // ttable [df]

/*
       Critical Values of Student's t
       .10     .05     .025    .01     .005    .0005  1-tail
 df    .20     .10     .050    .02     .010    .0010  2-tail
752   1.283   1.647   1.963   2.331   2.582    3.304

解读：在 5% 的显著性水平下，0.45188757 < 1.963 ，落入接受域，接受原假设。
*/

* 由于默认为双边检验，故可计算此 t 统计量对应的 p 值如下:

dis ttail(752,0.45188757)*2 // 0.65148029

/*
表示自由度为 752 的 t 分布比 0.45188757 更大的右侧尾部概率，正好是反向累积分布函数。
*/

* 单边检验

dis ttail(752, 0.45188757) // .32574014
/*
如果已知双边检验的p值，在做单边检验时(假设t统计量的符号 与替代假设的方向相同)，一般只需将双边检验的p值除以 2，即 可得到单边检验的 p 值，然后得到单边检验的结果。
*/

test expr = tenure // H_0: \beta_3 = \beta_4
/*
由于p值 = 0.8208，可轻松接受原假设。
*/

test expr + tenure = s // H_0: \beta_3 + \beta_4 = \beta_2
/*
由于 p 值 = 0.0031，故可在 1%的显著性水平上拒绝原假设，即认为\beta_3 + \beta_4 \neq \beta_2。
*/
```





